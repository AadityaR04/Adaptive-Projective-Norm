{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import itertools\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\") # For testing purposes, using CPU\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projective_Norm():\n",
    "    '''\n",
    "    Class to perform the optimization for calculating the minimum nuclear norm decomposition of a tensor\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    nature: str : nature of the tensor (symmetric or unsymmetric)\n",
    "    dim: tuple : dimensions of the tensor\n",
    "    device: torch.device : device to perform the optimization\n",
    "    type: str : type of the tensor (real or complex)\n",
    "    form: str : form of the input tensor (as a tensor or as a density matrix)\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    is_symmetric_tensor: Check if the tensor is symmetric\n",
    "    Parameter_Init: Initialize the parameters for the optimization\n",
    "    Reconstructed_State: Reconstruct the tensor from the parameters to get T' using einsum\n",
    "    Norm_Cost: Calculate the cost of the nuclear norm of the tensor. Minimum value of this cost is the nuclear norm of the tensor\n",
    "    Reconstruction_Cost: Calculate the cost of the reconstruction of the tensor. This cost is the mean squared error between the original tensor and the reconstructed tensor\n",
    "    Coefficent_Loss: Calculate the cost of the coefficents of the tensor. This is a square of number of terms greater than a threshold to make the coefficents sparse\n",
    "    Nuclear_Rank: Calculate the nuclear rank of the tensor. This is the number of non-zero coefficents in the tensor\n",
    "    forward: Calculate the total loss of the optimization\n",
    "    '''\n",
    "\n",
    "    def __init__(self, nature, dim, device, type, form):\n",
    "        '''\n",
    "        Initialize the parameters for the optimization\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        nature: str : nature of the tensor (symmetric or unsymmetric)\n",
    "        dim: tuple : dimensions of the tensor\n",
    "        device: torch.device : device to perform the optimization\n",
    "        type: str : type of the tensor (real or complex)\n",
    "        form: str : form of the input tensor (as a tensor or as a density matrix)\n",
    "\n",
    "        Initializes:\n",
    "        ------------\n",
    "        order: int : order of the tensor\n",
    "        dim_total: int : total dimnsionality of the tensor\n",
    "        r_max: int : maximum rank of the tensor\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        None\n",
    "        '''\n",
    "\n",
    "        self.nature = nature\n",
    "        # Check if the nature string is valid\n",
    "        if self.nature not in [\"symmetric\", \"unsymmetric\"]:\n",
    "            raise ValueError(\"Invalid nature string. Choose 'symmetric' or 'unsymmetric'\")\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.order = np.size(dim)\n",
    "        self.device = device\n",
    "        self.type = type\n",
    "        # Check if the type string is valid\n",
    "        if self.type not in [\"real\", \"complex\"]:\n",
    "            raise ValueError(\"Invalid type string. Choose 'real' or 'complex'\")\n",
    "        \n",
    "        self.form = form\n",
    "        # Check if the form string is valid\n",
    "        if self.form not in [\"tensor\", \"density_matrix\"]:\n",
    "            raise ValueError(\"Invalid form string. Choose 'tensor' or 'density_matrix'\")\n",
    "        \n",
    "        self.dim_total = np.prod(self.dim)\n",
    "\n",
    "        if self.form == \"tensor\":\n",
    "            self.r_max = int(self.dim_total/np.max(self.dim))\n",
    "        elif self.form == \"density_matrix\":\n",
    "            self.r_max = int(self.dim_total/np.max(self.dim))**2\n",
    "\n",
    "\n",
    "    def is_symmetric_tensor(self, T):\n",
    "        '''\n",
    "        Check if the tensor is symmetric\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        T: torch.tensor : tensor to check for symmetry\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        None: Raises an error if the tensor is not symmetric\n",
    "        '''\n",
    "\n",
    "        for i in range(self.order):\n",
    "            for j in range(i+1, self.order):\n",
    "                if self.dim[i] != self.dim[j]:\n",
    "                    raise ValueError(\"Input tensor is not symmetric as the dimensions are not equal\")\n",
    "\n",
    "        indices = list(range(self.order))\n",
    "        for perm in itertools.permutations(indices):\n",
    "            if not torch.allclose(T, T.permute(perm)):\n",
    "                raise ValueError(\"Input tensor is not symmetric as it is not invariant under permutation of indices\")\n",
    "            \n",
    "\n",
    "    def Parameter_Init(self):\n",
    "        '''\n",
    "        Initialize the parameters for the optimization\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        None\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        Param_dict: dict : dictionary of the parameters for the optimization\n",
    "        '''\n",
    "\n",
    "        Param_dict = {}\n",
    "\n",
    "        if self.form == \"tensor\":\n",
    "            if self.type == \"real\":\n",
    "                Param_dict[\"K\"] = torch.randn(self.r_max, requires_grad=True, device=self.device)\n",
    "\n",
    "            elif self.type == \"complex\":\n",
    "                Param_dict[\"K\"] = torch.randn((self.r_max, 2), requires_grad=True, device=self.device)\n",
    "\n",
    "            if self.type == \"real\":\n",
    "                if self.nature == \"unsymmetric\":        \n",
    "                    for i in range(self.order):\n",
    "                        Param_dict[\"x\"+str(i+1)] = torch.randn((self.dim[i], self.r_max), requires_grad=True, device=self.device)\n",
    "\n",
    "                elif self.nature == \"symmetric\":\n",
    "                    Param_dict[\"x\"] = torch.randn((self.dim[0], self.r_max), requires_grad=True, device=self.device)\n",
    "\n",
    "            elif self.type == \"complex\":\n",
    "                if self.nature == \"unsymmetric\":\n",
    "                    for i in range(self.order):\n",
    "                        Param_dict[\"x\"+str(i+1)] = torch.randn((self.dim[i], self.r_max, 2), requires_grad=True, device=self.device)\n",
    "\n",
    "                elif self.nature == \"symmetric\":\n",
    "                    Param_dict[\"x\"] = torch.randn((self.dim[0], self.r_max, 2), requires_grad=True, device=self.device)\n",
    "\n",
    "        elif self.form == \"density_matrix\":\n",
    "            if self.type == \"real\":\n",
    "                Param_dict[\"K\"] = torch.randn(self.r_max, requires_grad=True, device=self.device)\n",
    "                for i in range(self.order):\n",
    "                    Param_dict[\"x\"+str(i+1)] = torch.randn((self.dim[i], self.r_max), requires_grad=True, device=self.device) # Ket initialization\n",
    "                    Param_dict[\"y\"+str(i+1)] = torch.randn((self.dim[i], self.r_max), requires_grad=True, device=self.device) # Bra initialization\n",
    "\n",
    "            elif self.type == \"complex\":\n",
    "                Param_dict[\"K\"] = torch.randn(self.r_max, requires_grad=True, device=self.device)\n",
    "                for i in range(self.order):\n",
    "                    Param_dict[\"x\"+str(i+1)] = torch.randn((self.dim[i], self.r_max, 2), requires_grad=True, device=self.device)\n",
    "                    Param_dict[\"y\"+str(i+1)] = torch.randn((self.dim[i], self.r_max, 2), requires_grad=True, device=self.device)                \n",
    "\n",
    "        return Param_dict\n",
    "\n",
    "\n",
    "\n",
    "    def Reconstructed_State(self, Param_dict):\n",
    "        '''\n",
    "        Reconstruct the tensor from the parameters to get T' using einsum. Depending on the form of the input tensor, the reconstructed tensor is returned as either a tensor or a density matrix.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        Param_dict: dict : dictionary of the parameters for the optimization\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        T_recon: torch.tensor : reconstructed tensor T'\n",
    "        '''\n",
    "        \n",
    "        if self.form == \"tensor\":\n",
    "            if self.type == \"real\":\n",
    "                T_recon = torch.zeros(self.dim, dtype=torch.float32, device=self.device)\n",
    "                \n",
    "                if self.nature == \"unsymmetric\":\n",
    "                    for r in range(self.r_max):\n",
    "                        phi = torch.einsum('i,j->ij', Param_dict[\"x1\"][:, r], Param_dict[\"x2\"][:, r])\n",
    "                        for i in range(2, self.order):\n",
    "                            phi = torch.einsum('...ij, k->...ijk', phi, Param_dict[\"x\"+str(i+1)][:, r])\n",
    "                        T_recon = T_recon + Param_dict[\"K\"][r]*(phi/torch.norm(phi))\n",
    "\n",
    "                elif self.nature == \"symmetric\":\n",
    "                    for r in range(self.r_max):\n",
    "                        phi = torch.einsum('i,j->ij', Param_dict[\"x\"][:, r], Param_dict[\"x\"][:, r])\n",
    "                        for i in range(2, self.order):\n",
    "                            phi = torch.einsum('...ij, k->...ijk', phi, Param_dict[\"x\"][:, r])\n",
    "                        T_recon = T_recon + Param_dict[\"K\"][r]*(phi/torch.norm(phi))\n",
    "\n",
    "            elif self.type == \"complex\":\n",
    "                T_recon = torch.zeros(self.dim, dtype=torch.complex64, device=self.device)\n",
    "\n",
    "                if self.nature == \"unsymmetric\":\n",
    "                    for r in range(self.r_max):\n",
    "                        phi = torch.einsum('i,j->ij', Param_dict[\"x1\"][:, r, 0] + 1j*Param_dict[\"x1\"][:, r, 1], Param_dict[\"x2\"][:, r, 0] + 1j*Param_dict[\"x2\"][:, r, 1])\n",
    "                        for i in range(2, self.order):\n",
    "                            phi = torch.einsum('...ij, k->...ijk', phi, Param_dict[\"x\"+str(i+1)][:, r, 0] + 1j*Param_dict[\"x\"+str(i+1)][:, r, 1])\n",
    "                        T_recon = T_recon + (Param_dict[\"K\"][r, 0] + 1j*Param_dict[\"K\"][r, 1])*(phi/torch.norm(phi))\n",
    "\n",
    "                elif self.nature == \"symmetric\":\n",
    "                    for r in range(self.r_max):\n",
    "                        phi = torch.einsum('i,j->ij', Param_dict[\"x\"][:, r, 0] + 1j*Param_dict[\"x\"][:, r, 1], Param_dict[\"x\"][:, r, 0] + 1j*Param_dict[\"x\"][:, r, 1])\n",
    "                        for i in range(2, self.order):\n",
    "                            phi = torch.einsum('...ij, k->...ijk', phi, Param_dict[\"x\"][:, r, 0] + 1j*Param_dict[\"x\"][:, r, 1])\n",
    "                        T_recon = T_recon + (Param_dict[\"K\"][r, 0] + 1j*Param_dict[\"K\"][r, 1])*(phi/torch.norm(phi))\n",
    "\n",
    "        elif self.form == \"density_matrix\":\n",
    "            if self.type == \"real\":\n",
    "                T_recon = torch.zeros((self.dim_total, self.dim_total), dtype=torch.float32, device=self.device)\n",
    "\n",
    "                for r in range(self.r_max):\n",
    "                    phi_x = torch.einsum('i,j->ij', Param_dict[\"x1\"][:, r], Param_dict[\"x2\"][:, r])\n",
    "                    phi_y = torch.einsum('i,j->ij', Param_dict[\"y1\"][:, r], Param_dict[\"y2\"][:, r])\n",
    "                    for i in range(2, self.order):\n",
    "                        phi_x = torch.einsum('...ij, k->...ijk', phi_x, Param_dict[\"x\"+str(i+1)][:, r])\n",
    "                        phi_y = torch.einsum('...ij, k->...ijk', phi_y, Param_dict[\"y\"+str(i+1)][:, r])\n",
    "                    phi_x_flat = phi_x.view(-1, 1)\n",
    "                    phi_y_flat = phi_y.view(-1, 1)\n",
    "                    rho = phi_x_flat @ phi_y_flat.T\n",
    "                    T_recon = T_recon + Param_dict[\"K\"][r]*(rho/torch.norm(rho, p='fro'))\n",
    "\n",
    "            elif self.type == \"complex\":\n",
    "                T_recon = torch.zeros((self.dim_total, self.dim_total), dtype=torch.complex64, device=self.device)\n",
    "\n",
    "                for r in range(self.r_max):\n",
    "                    phi_x = torch.einsum('i,j->ij', Param_dict[\"x1\"][:, r, 0] + 1j*Param_dict[\"x1\"][:, r, 1], Param_dict[\"x2\"][:, r, 0] + 1j*Param_dict[\"x2\"][:, r, 1])\n",
    "                    phi_y = torch.einsum('i,j->ij', Param_dict[\"y1\"][:, r, 0] + 1j*Param_dict[\"y1\"][:, r, 1], Param_dict[\"y2\"][:, r, 0] + 1j*Param_dict[\"y2\"][:, r, 1])\n",
    "                    for i in range(2, self.order):\n",
    "                        phi_x = torch.einsum('...ij, k->...ijk', phi_x, Param_dict[\"x\"+str(i+1)][:, r, 0] + 1j*Param_dict[\"x\"+str(i+1)][:, r, 1])\n",
    "                        phi_y = torch.einsum('...ij, k->...ijk', phi_y, Param_dict[\"y\"+str(i+1)][:, r, 0] + 1j*Param_dict[\"y\"+str(i+1)][:, r, 1])\n",
    "                    phi_x_flat = phi_x.view(-1, 1)\n",
    "                    phi_y_flat = phi_y.view(-1, 1)\n",
    "                    rho = phi_x_flat @ phi_y_flat.T.conj()\n",
    "                    T_recon = T_recon + Param_dict[\"K\"][r]*(rho/torch.norm(rho, p='fro'))\n",
    "        \n",
    "        return T_recon\n",
    "    \n",
    "\n",
    "\n",
    "    def Norm_Cost(self, Param_dict):\n",
    "        '''\n",
    "        Calculate the cost of the nuclear norm of the tensor. Minimum value of this cost is the nuclear norm of the tensor\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        Param_dict: dict : dictionary of the parameters for the optimization\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        norm: torch.tensor : nuclear norm of the tensor\n",
    "        '''\n",
    "\n",
    "        norm = 0\n",
    "\n",
    "        if self.form == \"tensor\":\n",
    "            if self.type == \"real\":\n",
    "                if self.nature == \"unsymmetric\":\n",
    "                    for r in range(self.r_max):\n",
    "                        norm = norm + torch.abs(Param_dict[\"K\"][r])\n",
    "\n",
    "                elif self.nature == \"symmetric\":\n",
    "                    for r in range(self.r_max):\n",
    "                        norm = norm + torch.abs(Param_dict[\"K\"][r])\n",
    "\n",
    "            elif self.type == \"complex\":\n",
    "                if self.nature == \"unsymmetric\":\n",
    "                    for r in range(self.r_max):\n",
    "                        norm = norm + torch.norm(Param_dict[\"K\"][r, 0] + 1j*Param_dict[\"K\"][r, 1])\n",
    "\n",
    "                elif self.nature == \"symmetric\":\n",
    "                    for r in range(self.r_max):\n",
    "                        norm = norm + torch.norm(Param_dict[\"K\"][r, 0] + 1j*Param_dict[\"K\"][r, 1])\n",
    "\n",
    "        elif self.form == \"density_matrix\":\n",
    "            if self.type == \"real\":\n",
    "                for r in range(self.r_max):\n",
    "                    norm = norm + torch.abs(Param_dict[\"K\"][r])\n",
    "\n",
    "            elif self.type == \"complex\":\n",
    "                for r in range(self.r_max):\n",
    "                    norm = norm + torch.abs(Param_dict[\"K\"][r])\n",
    "\n",
    "        return norm\n",
    "\n",
    "\n",
    "    def Reconstruction_Cost(self, T, T_recon):\n",
    "        '''\n",
    "        Calculate the cost of the reconstruction of the tensor. This cost is the mean squared error between the original tensor and the reconstructed tensor\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        T: torch.tensor : original tensor T\n",
    "        T_recon: torch.tensor : reconstructed tensor T'\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        loss: torch.tensor : mean squared error between the original tensor and the reconstructed tensor (||T - T'||^2)\n",
    "        '''\n",
    "        \n",
    "        if self.type == \"real\":\n",
    "            return F.mse_loss(T, T_recon)\n",
    "        elif self.type == \"complex\":\n",
    "            return F.mse_loss(T.real, T_recon.real) + F.mse_loss(T.imag, T_recon.imag)\n",
    "\n",
    "\n",
    "    def Coefficent_Loss(self, Param_dict):\n",
    "        '''\n",
    "        Calculate the cost of the coefficents of the tensor. This is a square of number of terms greater than a threshold to make the coefficents sparse\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        Param_dict: dict : dictionary of the parameters for the optimization\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        torch.tensor : Square of number of terms greater than a threshold\n",
    "        '''\n",
    "        if self.type == \"real\" or self.form == \"density_matrix\":\n",
    "            return torch.sum(torch.abs(Param_dict[\"K\"]) > 1e-3)**2\n",
    "        elif self.type == \"complex\" and self.form == \"tensor\":\n",
    "            return torch.sum(torch.sqrt(Param_dict[\"K\"][:, 0]**2 + Param_dict[\"K\"][:, 1]**2) > 1e-3)**2\n",
    "\n",
    "\n",
    "\n",
    "    def Nuclear_Rank(self, Param_dict, threshold = 1 * 1e-1):\n",
    "        '''\n",
    "        Calculate the nuclear rank of the tensor. This is the number of non-zero coefficents in the tensor\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        Param_dict: dict : dictionary of the parameters for the optimization\n",
    "        threshold: float : threshold to consider a coefficent as non-zero (default = 1e-2)\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        non_zero_weights: int : number of non-zero coefficents in the tensor\n",
    "        '''\n",
    "\n",
    "        non_zero_weights = 0\n",
    "        if self.type == \"real\" or self.form == \"density_matrix\":\n",
    "            non_zero_weights = torch.sum(torch.abs(Param_dict[\"K\"]) > threshold)\n",
    "\n",
    "        elif self.type == \"complex\" and self.form == \"tensor\":\n",
    "            non_zero_weights = torch.sum((torch.sqrt(Param_dict[\"K\"][:, 0]**2 + Param_dict[\"K\"][:, 1]**2)) > threshold)\n",
    "\n",
    "        return non_zero_weights\n",
    "\n",
    "  \n",
    "    \n",
    "    def forward(self, T, params):\n",
    "        '''\n",
    "        Calculate the total loss of the optimization\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        T: torch.tensor : original tensor T\n",
    "        params: dict : dictionary of the parameters for the optimization\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        loss: torch.tensor : total loss of the optimization\n",
    "        '''\n",
    "\n",
    "        k_1 = 1e12 * self.dim_total # Reconstruction Cost Coefficent\n",
    "        k_2 = 1e3 * self.dim_total # Coefficent Cost Coefficent\n",
    "        k_3 = 1e6* self.dim_total # Norm Cost Coefficent\n",
    "        \n",
    "        # params = self.Parameter_Init()\n",
    "        T_recon = self.Reconstructed_State(params)\n",
    "        coeff_loss = self.Coefficent_Loss(params)\n",
    "        recon_loss = self.Reconstruction_Cost(T, T_recon)\n",
    "        norm_loss = self.Norm_Cost(params)\n",
    "\n",
    "        loss = k_1*recon_loss + k_2*coeff_loss + k_3*norm_loss\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InputState(dim, type, form, device):\n",
    "\n",
    "    d_type = torch.float32\n",
    "    \n",
    "    if type == \"complex\":\n",
    "        d_type = torch.complex64\n",
    "    A = torch.zeros(dim, dtype=d_type)\n",
    "\n",
    "    #### Customize the input state here ####\n",
    "    \n",
    "    if form == \"density_matrix\":\n",
    "        A = A.view(np.prod(dim), 1) @ A.view(1, np.prod(dim)).conj()\n",
    "    \n",
    "    A[0, 0] = 1/np.sqrt(2)\n",
    "    A[0, 1] = 1/np.sqrt(2)\n",
    "\n",
    "    ########################################\n",
    "    return A.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss = []\n",
    "N_Norm = []\n",
    "\n",
    "EPOCHS = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type = \"complex\"\n",
    "nature = \"unsymmetric\"\n",
    "dim = (2,2)\n",
    "form = \"tensor\"   # form is either 'tensor' or 'density_matrix'\n",
    "\n",
    "T = InputState(dim, type, form, device)\n",
    "\n",
    "projective_norm = Projective_Norm(nature, dim, device, type, form)\n",
    "\n",
    "Param_dict = projective_norm.Parameter_Init()\n",
    "\n",
    "if nature == \"symmetric\" and form == \"tensor\":\n",
    "    projective_norm.is_symmetric_tensor(T)\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(Param_dict.values(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=10000, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "pbar = tqdm(range(EPOCHS), desc=\"Projective Norm convergence\")\n",
    "while epoch < EPOCHS:\n",
    "    optimizer.zero_grad()\n",
    "    loss = projective_norm.forward(T, Param_dict)\n",
    "    loss.backward()\n",
    "    Loss.append(loss.item())\n",
    "    N_Norm.append(projective_norm.Norm_Cost(Param_dict).item())\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    # If calculated nuclear norm is too large in iteration 0, try another set of parameters and keep repeating the loop until the nuclear norm is small enough\n",
    "    if epoch == 0 and projective_norm.Norm_Cost(Param_dict).item() > np.prod(dim)/np.max(dim):\n",
    "        print(\"Projective norm is too large at epoch 0, trying another set of parameters for faster convergence\")\n",
    "        Param_dict = projective_norm.Parameter_Init()\n",
    "        optimizer = torch.optim.Adam(Param_dict.values(), lr=learning_rate)\n",
    "        epoch = 0\n",
    "        # Go back to the start of the loop\n",
    "        continue\n",
    "    else:\n",
    "        if scheduler.get_last_lr()[0] >= 1e-4:\n",
    "            scheduler.step()\n",
    "\n",
    "        if epoch % 100 == 0 or epoch == EPOCHS - 1:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}, Reconstruction loss: {projective_norm.Reconstruction_Cost(T, projective_norm.Reconstructed_State(Param_dict)).item()}, Projective Norm: {projective_norm.Norm_Cost(Param_dict).item()}, learning rate: {scheduler.get_last_lr()[0]}\")\n",
    "\n",
    "    epoch += 1\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Loss[1000:])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Value of the Objective Function\")\n",
    "plt.title(\"Objective Function vs Epochs for calculating the nuclear norm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = 1.5\n",
    "state = 'separable_state'\n",
    "dimension = ''\n",
    "for idx, d in enumerate(dim):\n",
    "    if idx == len(dim) - 1:\n",
    "        dimension += str(d)\n",
    "    else:\n",
    "        dimension += str(d) + '-'\n",
    "filename =  './Pickle_files/' + state + '-' + dimension + '-' + type + '-' + nature + '-' + form + '.pkl'\n",
    "\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(N_Norm, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytical_rank = 1\n",
    "nuclear_rank = projective_norm.Nuclear_Rank(Param_dict)\n",
    "\n",
    "print(f\"Analytical Rank: {analytical_rank}\")\n",
    "print(f\"Nuclear Rank: {nuclear_rank}\")\n",
    "\n",
    "analytical_projective_norm = 1\n",
    "final_projective_norm = N_Norm[-1]\n",
    "\n",
    "print(f\"Analytical Nuclear Norm: {analytical_projective_norm}\")\n",
    "print(f\"Calculated Nuclear Norm: {final_projective_norm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "ax.plot(N_Norm, label=\"Calculated Projective Norm\", color='orange', linestyle='-', linewidth=3)\n",
    "ax.plot([analytical_projective_norm]*EPOCHS, label=\"Analytical Projective Norm\", color='blue', linestyle='--', linewidth=1.5)\n",
    "\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "\n",
    "# Uncomment the following lines to display the analytical and calculated projective norms and nuclear ranks in the plot\n",
    "ax.text(1.02, 0.5,\n",
    "        f'Analytical Projective Norm: {round(analytical_projective_norm, 4)}\\n\\nCalculated Projective Norm: {round(final_projective_norm, 4)}\\n------------------------------------------------\\nAnalytical Nuclear Rank: {analytical_rank}\\n\\nCalculated Nuclear Rank: {nuclear_rank}',\n",
    "        transform=ax.transAxes, fontsize=12, verticalalignment='center')\n",
    "\n",
    "## Uncomment the following lines to display the analytical and calculated projective norms in the plot\n",
    "# ax.text(1.02, 0.5, \n",
    "#         f'Analytical Projective Norm: {round(analytical_projective_norm, 4)}\\n\\nCalculated Projective Norm: {round(N_Norm[-1], 4)}', \n",
    "#         transform=ax.transAxes, fontsize=12, verticalalignment='center')\n",
    "\n",
    "## Uncomment the following lines to display the calculated nuclear norm in the plot\n",
    "# ax.text(1.02, 0.5, \n",
    "#         f'Calculated Nuclear Norm: {round(N_Norm[-1], 5)}', \n",
    "#         transform=ax.transAxes, fontsize=12, verticalalignment='center')\n",
    "\n",
    "ax.set_xlabel(\"Number of Iterations\", fontsize=14)\n",
    "ax.set_ylabel(\"Value of the Projective Norm\", fontsize=14)\n",
    "fig.suptitle(r\"Projective Norm of the 2 qubit separable state $|\\psi\\rangle = \\frac{1}{\\sqrt{2}}\\left(|00\\rangle + |01\\rangle\\right)$ in $\\mathbb{C}$\", fontsize=13, x=0.57, y=0.95)\n",
    "\n",
    "ax.grid()\n",
    "ax.legend(fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "fig_filename = \"./Results/\" + state + \"_\" + dimension + \"_\" + type + \"_\" + nature + \".png\"\n",
    "fig.savefig(fig_filename, dpi=150, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
